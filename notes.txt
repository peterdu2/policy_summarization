Todos:
    - AST Sim clone state function
    - locations of runner (need to be at root of ast folder)
    - location of data folder with dsrnn model (needs relative path from ast batch runner)
      temporarily copied data folder to ast folder

    - add to python path for runner to find crowd sim packages:
      export PYTHONPATH=$PYTHONPATH:/home/peterdu2/policy_summarization/AST_CrowdNav


MCTS Iteration info:
DPWParams:
  d = max search depth (ie max_path_length)
  gamma = discount factor
  ec = exporation bonus
  k = The constraint parameter used in DPW: |N(s,a)|<=kN(s)^alpha.
  alpha = The constraint parameter used in DPW: |N(s,a)|<=kN(s)^alpha.

  n = maximum number of iterations
    = 2 * max(n_itr * log_interval // max_path_length ** 2, 1)
  n_iter, log_interval, and max_path_length provided by algo args in runner.py


Selected policies:
5600.pt: episode mean reward: 2.5661718399999995
8100.pt: episode mean reward: 6.61834936
8800.pt: episode mean reward: 9.10117969
13200.pt: episode mean reward: 9.81915886

Final policies: 
** 14000.pt: episode mean reward: 10.07230044  ID = Policy A
20600.pt: episode mean reward: 16.52406858  ID = Policy B
30800.pt: episode mean reward: 20.23076747  ID = Policy C

Testing policies:
13000.pt: episode mean reward: 9.144999219999999
34400.pt: episode mean reward: 21.548123050000004 


Results:

dsrnn0 ['30800.pt', '20600.pt']:
  - Original circular human position

dsrnn1 ['14000.pt', '20600.pt']:
  - Updated (less circular) human initial positions

dsrnn2 ['13200.pt', '20600.pt']:
  - Updated (less circular) human initial positions
  - Probably will discard this model (will not use 13200.pt)

dsrnn3 ['20600.pt', '30800.pt']:
  - Updated (less circular) human initial positions
  - Rerun of dsrnn0 but with updated human initial positions 

dsrnn4 ['20600.pt', '30800.pt']:
  - Slightly changed human positions (humans 7 and 8 are more spread out)
  - Reduce bouncing behaviour near the goal caused by humans 7 and 8


Target set:
  Human position set 1 (Collision Reward = 100):
  dsrnn1  ['14000.pt', '20600.pt']
  dsrnn3  ['20600.pt', '30800.pt']
  dsrnn5  ['14000.pt', '30800.pt']

  Human position set 2 (Collision Reward = 100):
  dsrnn6  ['14000.pt', '20600.pt']
  dsrnn7  ['20600.pt', '30800.pt']
  dsrnn8  ['14000.pt', '30800.pt']

  Human position set 3 (Collision Reward = 25):
  dsrnn9 ['14000.pt', '20600.pt']
  dsrnn10  ['20600.pt', '30800.pt']
  dsrnn11  ['14000.pt', '30800.pt']

  Human position set 4 (Collision Reward = 0):
  dsrnn12  ['14000.pt', '20600.pt']
  dsrnn13  ['20600.pt', '30800.pt']
  dsrnn14  ['14000.pt', '30800.pt']

  Human position set 5 (Collision Reward = 0, new reward goal policy):
  dsrnn15  ['14000.pt', '20600.pt']
  dsrnn16  ['20600.pt', '30800.pt']
  dsrnn17  ['14000.pt', '30800.pt']

New policy set
  Human position set 6 (Collision Reward = 0, original reward): *Dervied from PS1 
  dsrnn18  ['13000.pt', '20600.pt']
  dsrnn19  ['20600.pt', '30800.pt']
  dsrnn20  ['13000.pt', '30800.pt']